{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas is already installed.\n",
      "numpy is already installed.\n",
      "matplotlib is already installed.\n",
      "gym is already installed.\n",
      "gymnasium is already installed.\n",
      "stable_baselines3 is already installed.\n",
      "Installing shimmy...\n",
      "Collecting shimmy\n",
      "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/homebrew/lib/python3.11/site-packages (from shimmy) (1.24.3)\n",
      "Requirement already satisfied: gymnasium>=1.0.0a1 in /opt/homebrew/lib/python3.11/site-packages (from shimmy) (1.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/homebrew/lib/python3.11/site-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
      "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: shimmy\n",
      "Successfully installed shimmy-2.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "required_packages = [\n",
    "    \"pandas\", \"numpy\", \"matplotlib\", \"gym\", \"gymnasium\", \"stable_baselines3\", \"shimmy\"\n",
    "]\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Installs a package using pip if it's not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in required_packages:\n",
    "    install_package(package)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Reinforcement Learning for the Free Kick Scenario\n",
    "\n",
    "In this section, we integrate a Reinforcement Learning (RL) model into our football simulation environment to enhance the decision-making of players during a free kick scenario. The goal is to train an agent that can effectively navigate the complexities of football tactics, such as dribbling, passing, shooting, and defending.\n",
    "\n",
    "Reinforcement learning allows to simulate intelligent agents that can learn optimal strategies through trial and error. By interacting with the environment, the agent will receive rewards or penalties based on its actions, enabling it to improve over time. The advantage of RL is that the agent can learn autonomously without explicit programming of every action, which is essential for simulating the dynamic and evolving nature of football matches.\n",
    "\n",
    "In our case:\n",
    "- **Attackers** can move, pass the ball, shoot, or dribble, depending on their possession of the ball.\n",
    "- **Defenders** are focused on positioning and moving to block the ball or intercept passes.\n",
    "- **The Free Kick** starts with the kicker deciding whether to pass or shoot, and after the first step, all players can perform any action available.\n",
    "\n",
    "### Using PPO for Training\n",
    "\n",
    "For this problem, we use the **Proximal Policy Optimization (PPO)** algorithm from **Stable-Baselines3**. PPO is an on-policy RL algorithm that performs well in environments with discrete action spaces, making it suitable for our football simulation.\n",
    "\n",
    "The model will be trained using the following process:\n",
    "1. **Training**: The agent interacts with the environment, taking actions based on its current policy and receiving feedback in the form of rewards.\n",
    "2. **Evaluation**: After training, we test the agent to observe how well it performs, ensuring that it has learned to make effective decisions in the context of a free kick.\n",
    "\n",
    "In the code below:\n",
    "- The environment (`FreeKickEnv`) is set up to allow players to perform actions like dribbling, passing, or shooting.\n",
    "- A PPO agent is initialized and trained to interact with this environment, learning from its actions and the rewards received.\n",
    "- During testing, the agent's actions and the environment's state (including player positions and ball movement) are visualized.\n",
    "\n",
    "Now, let's look at the complete implementation of the RL training and testing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freeKickEnv import FreeKickEnv\n",
    "from drawPitch import draw_pitch\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import time\n",
    "import random\n",
    "from IPython.display import display, clear_output #type: ignore\n",
    "from utils import to_field, ACTION_NAMES\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Training the model...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.7     |\n",
      "|    ep_rew_mean     | 20.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 3467     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 25.2        |\n",
      "|    ep_rew_mean          | 25.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2123        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008360265 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.52        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 48          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 33.7        |\n",
      "|    ep_rew_mean          | 33.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1893        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010033874 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    value_loss           | 35.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 44.7         |\n",
      "|    ep_rew_mean          | 44.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1815         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069928234 |\n",
      "|    clip_fraction        | 0.0788       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.641       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.7         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0174      |\n",
      "|    value_loss           | 48           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 56.6        |\n",
      "|    ep_rew_mean          | 56.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1810        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008023769 |\n",
      "|    clip_fraction        | 0.0825      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.334       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    value_loss           | 54.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Start simulation loop with the trained agent\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Get the action from the model (using the trained agent)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Apply the action to the environment\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     obs, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/stable_baselines3/common/policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "# Step 1: Create the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# Step 2: Initialize the PPO agent\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Step 3: Train the agent\n",
    "print(\"Training the model...\")\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# TESTING THE MODEL\n",
    "\n",
    "# Create a persistent figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Reset the environment and get the initial observation\n",
    "obs = env.reset()  # Now this will return a single observation (not a tuple)\n",
    "done = False\n",
    "\n",
    "# Start simulation loop with the trained agent\n",
    "while not done:\n",
    "    # Get the action from the model (using the trained agent)\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "    # Apply the action to the environment\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    # Render the environment\n",
    "    ax.clear()\n",
    "    ax.imshow(env.render(mode='rgb_array'))  # Display environment image\n",
    "\n",
    "    # Show the plot (note that this only works in Jupyter notebooks)\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "    # Print action and reward\n",
    "    print(f\"Action: {action} | Reward: {reward}\")\n",
    "\n",
    "    # Delay between frames\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    if done:\n",
    "        print(\"Simulation ended.\")\n",
    "        break\n",
    "\n",
    "# Turn off interactive mode and close the figure\n",
    "plt.ioff()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
